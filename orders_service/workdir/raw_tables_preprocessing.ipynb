{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b99ed4-e24e-4c9d-8d15-d41d673a997f",
   "metadata": {},
   "source": [
    "# Preprocess Raw Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47939d15-ceb6-4fb0-913a-e60701d4ac62",
   "metadata": {},
   "source": [
    "> N.B. para ejecutar el notebook es necesaro modificar el magic_command de iam_role. Cada cuenta tiene uno asociado, navegar a la pestaÃ±a IAM/Roles/LabRole dentro de AWs y copiar ARN como se indica en la imagen adjuntada.\n",
    ">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a428911-8751-4478-8295-9d744f77b577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "It looks like there is a newer version of the kernel available. The latest version is 1.0.6 and you have 1.0.4 installed.\n",
      "Please run `pip install --upgrade aws-glue-sessions` to upgrade your kernel\n",
      "Current iam_role is None\n",
      "iam_role has been set to arn:aws:iam::484183516222:role/LabRole.\n",
      "Previous region: None\n",
      "Setting new region to: us-east-1\n",
      "Region is set to: us-east-1\n",
      "Previous number of workers: None\n",
      "Setting new number of workers to: 2\n",
      "Current idle_timeout is None minutes.\n",
      "idle_timeout has been set to 60 minutes.\n"
     ]
    }
   ],
   "source": [
    "%iam_role arn:aws:iam::484183516222:role/LabRole\n",
    "%region us-east-1\n",
    "%number_of_workers 2\n",
    "%idle_timeout 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cbc1f9-d795-490f-b7e8-91b0f0bdf8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Session Type: etl\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: 626e67a8-c65b-426b-a921-efaee663dcc2\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 1.0.4\n",
      "--enable-glue-datacatalog true\n",
      "Waiting for session 626e67a8-c65b-426b-a921-efaee663dcc2 to get into ready status...\n",
      "Session 626e67a8-c65b-426b-a921-efaee663dcc2 has been created.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f2823bfa290>\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2299c-87a5-4202-9c5e-54b232d5466f",
   "metadata": {},
   "source": [
    "## 1. imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d55e9a2-6888-49f8-8ee5-93b681496a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33a62dc5-8261-4449-a6b9-ecfc29bf308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"s3://vrpoptimiserplatform\"\n",
    "RAW = \"raw\"\n",
    "ORDERS = \"orders\"\n",
    "\n",
    "BRONZE = \"bronze\"\n",
    "SILVER = \"silver\"\n",
    "GOLD = \"gold\"\n",
    "\n",
    "ADDRESS_DATA = \"address_data.json\"\n",
    "CLIENTS_DATA = \"client_data.json\"\n",
    "\n",
    "ADDRESS_TABLE = \"address_table\"\n",
    "CLIENTS_TABLE = \"clients_table\"\n",
    "CLIENTS_ADDRESS_TABLE = \"clients_address_table\"\n",
    "\n",
    "RAW_ADDRESS_PATH = os.path.join(BUCKET_NAME, RAW, ADDRESS_DATA)\n",
    "RAW_CIENTS_PATH = os.path.join(BUCKET_NAME, RAW, CLIENTS_DATA)\n",
    "\n",
    "BRONZE_ADDRESS_PATH = os.path.join(BUCKET_NAME, ORDERS, BRONZE, ADDRESS_TABLE)\n",
    "BRONZE_CLIENTS_PATH = os.path.join(BUCKET_NAME, ORDERS, BRONZE, CLIENTS_TABLE)\n",
    "\n",
    "SILVER_ADDRESS_PATH = os.path.join(BUCKET_NAME, ORDERS, SILVER, ADDRESS_TABLE)\n",
    "SILVER_CLIENTS_PATH = os.path.join(BUCKET_NAME, ORDERS, SILVER, CLIENTS_TABLE)\n",
    "\n",
    "GOLD_CLIENTS_ADDRESS_PATH = os.path.join(BUCKET_NAME, ORDERS, GOLD, CLIENTS_ADDRESS_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8f3f6cd-b610-4acf-adcd-34a4e81cac3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_json_to_df(file_path, schema=None):\n",
    "    \"\"\"\n",
    "    Read JSON file into DataFrame.\n",
    "    \n",
    "    :param file_path: Path to the JSON file.\n",
    "    :param schema: Optional schema to enforce while reading.\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # Validate file_path\n",
    "    if not isinstance(file_path, str) or not file_path:\n",
    "        raise ValueError(\"Invalid file path provided.\")\n",
    "\n",
    "    # Read DataFrame from JSON\n",
    "    reader = spark.read.format(\"json\").option(\"multiLine\", \"true\").option(\"mode\", \"PERMISSIVE\")\n",
    "    \n",
    "    if schema:\n",
    "        reader = reader.schema(schema)\n",
    "    else:\n",
    "        reader = reader.option(\"inferSchema\", \"true\")\n",
    "    \n",
    "    try:\n",
    "        df = reader.load(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading JSON file: {str(e)}\")\n",
    "\n",
    "def write_df_to_metastore(df, file_path, table_name, partition_by=None, mode=\"overwrite\"):\n",
    "    \"\"\"\n",
    "    Write DataFrame to Parquet file and save it as a table in the metastore.\n",
    "    \n",
    "    :param df: DataFrame to be written.\n",
    "    :param file_path: Path where the Parquet file will be saved.\n",
    "    :param table_name: Name of the table to save.\n",
    "    :param partition_by: Column(s) to partition by.\n",
    "    :param mode: Write mode, default is 'overwrite'. Other options are 'append', 'ignore', 'error'.\n",
    "    \"\"\"\n",
    "    # Validate parameters\n",
    "    if not file_path or not isinstance(file_path, str):\n",
    "        raise ValueError(\"Invalid file path provided.\")\n",
    "    if not table_name or not isinstance(table_name, str):\n",
    "        raise ValueError(\"Invalid table name provided.\")\n",
    "    if partition_by and not isinstance(partition_by, (str, list)):\n",
    "        raise ValueError(\"Partition by should be a string or a list of strings.\")\n",
    "\n",
    "    writer = df.write.format(\"parquet\").mode(mode).option(\"path\", file_path)\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def transform_clients_bronze_to_silver(clients_df):\n",
    "    \"\"\"\n",
    "    Transform Bronze (raw) Clients DataFrame to Silver (cleaned) DataFrame.\n",
    "    \n",
    "    :param clients_df: Clients DataFrame.\n",
    "    :return: Cleaned Clients DataFrame.\n",
    "    \"\"\"\n",
    "    # Example transformations: Filtering active clients, renaming columns, etc.\n",
    "    clients_silver_df = (\n",
    "        clients_df\n",
    "        .filter(f.col(\"status\") == \"active\")\n",
    "    )\n",
    "    return clients_silver_df\n",
    "\n",
    "def transform_addresses_bronze_to_silver(addresses_df):\n",
    "    \"\"\"\n",
    "    Transform Bronze (raw) Addresses DataFrame to Silver (cleaned) DataFrame.\n",
    "    \n",
    "    :param addresses_df: Addresses DataFrame.\n",
    "    :return: Cleaned Addresses DataFrame.\n",
    "    \"\"\"\n",
    "    # Cast coordenates format to float\n",
    "    addresses_silver_df = (\n",
    "        addresses_df\n",
    "        #.filter(col(\"house_number\") != \"\")\n",
    "        .withColumn(\"lat\", f.col(\"lat\").cast(\"float\"))\n",
    "        .withColumn(\"lon\", f.col(\"lon\").cast(\"float\"))\n",
    "    )\n",
    "    return addresses_silver_df\n",
    "\n",
    "\n",
    "def transform_clients_addresses_silver_to_gold(clients_silver_df, addresses_silver_df):\n",
    "    \"\"\"\n",
    "    Transform Silver (cleaned) Clients and Addresses DataFrames to Gold (aggregated/enriched) DataFrame.\n",
    "    \n",
    "    :param clients_silver_df: Cleaned Clients DataFrame.\n",
    "    :param addresses_silver_df: Cleaned Addresses DataFrame.\n",
    "    :return: Enriched DataFrame combining both clients and addresses.\n",
    "    \"\"\"\n",
    "    # Example aggregation: Joining clients with their addresses\n",
    "    gold_df = (\n",
    "        clients_silver_df\n",
    "        .join(addresses_silver_df, on=\"client_id\", how=\"right\")\n",
    "        .dropna(subset=['client_id'])\n",
    "    )\n",
    "    return gold_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3966786-7aa7-4a8b-8901-76938b8888d7",
   "metadata": {},
   "source": [
    "## 2. Medallion Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b71d1-756f-4823-964d-b0d2431aef03",
   "metadata": {},
   "source": [
    "### 2.1 Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc7a55b6-d181-4646-98c6-a6764c218bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the clients JSON structure\n",
    "clients_schema = t.StructType([\n",
    "    t.StructField(\"client_id\", t.StringType(), False),\n",
    "    t.StructField(\"first_name\", t.StringType(), False),\n",
    "    t.StructField(\"last_name\", t.StringType(), False),\n",
    "    t.StructField(\"email\", t.StringType(), True),\n",
    "    t.StructField(\"phone_number\", t.StringType(), True),\n",
    "    t.StructField(\"date_of_birth\", t.DateType(), True),\n",
    "    t.StructField(\"gender\", t.StringType(), True),\n",
    "    t.StructField(\"occupation\", t.StringType(), True),\n",
    "    t.StructField(\"created_at\", t.DateType(), True),\n",
    "    t.StructField(\"updated_at\", t.DateType(), True),\n",
    "    t.StructField(\"status\", t.StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the addresses JSON structure\n",
    "addresses_schema = t.StructType([\n",
    "    t.StructField(\"client_id\", t.StringType(), False),\n",
    "    t.StructField(\"address_id\", t.StringType(), False),\n",
    "    t.StructField(\"neighborhood\", t.StringType(), True),\n",
    "    t.StructField(\"coordinates\", t.ArrayType(t.DoubleType()), True),\n",
    "    t.StructField(\"road\", t.StringType(), True),\n",
    "    t.StructField(\"house_number\", t.StringType(), True),\n",
    "    t.StructField(\"suburb\", t.StringType(), True),\n",
    "    t.StructField(\"city_district\", t.StringType(), True),\n",
    "    t.StructField(\"state\", t.StringType(), True),\n",
    "    t.StructField(\"postcode\", t.StringType(), True),\n",
    "    t.StructField(\"country\", t.StringType(), True),\n",
    "    t.StructField(\"lat\", t.StringType(), True),\n",
    "    t.StructField(\"lon\", t.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42300c19-3aba-4e6f-840b-904dd75f8d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_address_raw = read_json_to_df(RAW_ADDRESS_PATH, addresses_schema)\n",
    "df_clients_raw = read_json_to_df(RAW_CIENTS_PATH, clients_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d7f486c-fb34-4881-a75c-fe2615632edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_df_to_metastore(df_address_raw, BRONZE_ADDRESS_PATH, \"bronze_address_table\")\n",
    "write_df_to_metastore(df_clients_raw, BRONZE_CLIENTS_PATH, \"bronze_clients_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8e2bd-a15c-4798-b2aa-269fd099710e",
   "metadata": {},
   "source": [
    "### 2.2 Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccc7a764-746e-4d97-ab87-f51d5a89f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_address_bronze = spark.table(\"bronze_address_table\")\n",
    "df_clients_bronze = spark.table(\"bronze_clients_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a3de0d8-5a88-4f6f-8bfc-2483c1a693c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_df_to_metastore(transform_addresses_bronze_to_silver(df_address_bronze), SILVER_ADDRESS_PATH, \"silver_address_table\")\n",
    "write_df_to_metastore(transform_clients_bronze_to_silver(df_clients_bronze), SILVER_CLIENTS_PATH, \"silver_clients_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ea14a-e5a6-4257-8e46-35bbb8d86b35",
   "metadata": {},
   "source": [
    "### 2.3 Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e0d155f-d848-42d6-ada6-73471bce7ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_address_silver = spark.table(\"silver_address_table\")\n",
    "df_clients_silver = spark.table(\"silver_clients_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "266c0005-8405-4d94-9ec2-204c195c94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_df_to_metastore(transform_clients_addresses_silver_to_gold(df_clients_silver, df_address_silver), GOLD_CLIENTS_ADDRESS_PATH, \"gold_clients_address_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
