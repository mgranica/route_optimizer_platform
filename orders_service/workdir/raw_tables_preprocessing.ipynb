{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b99ed4-e24e-4c9d-8d15-d41d673a997f",
   "metadata": {},
   "source": [
    "# Preprocess Raw Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47939d15-ceb6-4fb0-913a-e60701d4ac62",
   "metadata": {},
   "source": [
    "> N.B. para ejecutar el notebook es necesaro modificar el magic_command de iam_role. Cada cuenta tiene uno asociado, navegar a la pestaÃ±a IAM/Roles/LabRole dentro de AWs y copiar ARN como se indica en la imagen adjuntada.\n",
    ">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a428911-8751-4478-8295-9d744f77b577",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "It looks like there is a newer version of the kernel available. The latest version is 1.0.6 and you have 1.0.4 installed.\n",
      "Please run `pip install --upgrade aws-glue-sessions` to upgrade your kernel\n",
      "Current iam_role is None\n",
      "iam_role has been set to arn:aws:iam::484183516222:role/LabRole.\n",
      "Previous region: None\n",
      "Setting new region to: us-east-1\n",
      "Region is set to: us-east-1\n",
      "Previous number of workers: None\n",
      "Setting new number of workers to: 2\n",
      "Current idle_timeout is None minutes.\n",
      "idle_timeout has been set to 60 minutes.\n",
      "The following configurations have been updated: {'--conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog', '--datalake-formats': 'delta'}\n"
     ]
    }
   ],
   "source": [
    "%iam_role arn:aws:iam::484183516222:role/LabRole\n",
    "%region us-east-1\n",
    "%number_of_workers 2\n",
    "%idle_timeout 60\n",
    "%%configure \n",
    "{\n",
    "  \"--conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "  \"--datalake-formats\": \"delta\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cbc1f9-d795-490f-b7e8-91b0f0bdf8b4",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Session Type: etl\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: 50054946-2353-4a28-8637-ccb4be7f329c\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 1.0.4\n",
      "--enable-glue-datacatalog true\n",
      "--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "--datalake-formats delta\n",
      "Waiting for session 50054946-2353-4a28-8637-ccb4be7f329c to get into ready status...\n",
      "Session 50054946-2353-4a28-8637-ccb4be7f329c has been created.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f304ab74c70>\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2299c-87a5-4202-9c5e-54b232d5466f",
   "metadata": {},
   "source": [
    "## 1. imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d55e9a2-6888-49f8-8ee5-93b681496a98",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a62dc5-8261-4449-a6b9-ecfc29bf308f",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"s3://vrpoptimiserplatform\"\n",
    "RAW = \"raw\"\n",
    "ORDERS = \"orders\"\n",
    "\n",
    "BRONZE = \"bronze\"\n",
    "SILVER = \"silver\"\n",
    "GOLD = \"gold\"\n",
    "\n",
    "ADDRESS_DATA = \"address_data.json\"\n",
    "CLIENTS_DATA = \"client_data.json\"\n",
    "PRODUCTS_DATA = \"marketing_sample_for_amazon_com-ecommerce.csv\"\n",
    "\n",
    "ADDRESS_TABLE = \"address_table\"\n",
    "CLIENTS_TABLE = \"clients_table\"\n",
    "CLIENTS_ADDRESS_TABLE = \"clients_address_table\"\n",
    "PRODUCTS_TABLE = \"products_table\"\n",
    "\n",
    "RAW_ADDRESS_PATH = os.path.join(BUCKET_NAME, RAW, ADDRESS_DATA)\n",
    "RAW_CIENTS_PATH = os.path.join(BUCKET_NAME, RAW, CLIENTS_DATA)\n",
    "RAW_PRODUCTS_PATH = os.path.join(BUCKET_NAME, RAW, PRODUCTS_DATA)\n",
    "\n",
    "BRONZE_ADDRESS_PATH = os.path.join(BUCKET_NAME, ORDERS, BRONZE, ADDRESS_TABLE)\n",
    "BRONZE_CLIENTS_PATH = os.path.join(BUCKET_NAME, ORDERS, BRONZE, CLIENTS_TABLE)\n",
    "BRONZE_PRODUCTS_PATH = os.path.join(BUCKET_NAME, ORDERS, BRONZE, PRODUCTS_TABLE)\n",
    "\n",
    "SILVER_ADDRESS_PATH = os.path.join(BUCKET_NAME, ORDERS, SILVER, ADDRESS_TABLE)\n",
    "SILVER_CLIENTS_PATH = os.path.join(BUCKET_NAME, ORDERS, SILVER, CLIENTS_TABLE)\n",
    "SILVER_PRODUCTS_PATH = os.path.join(BUCKET_NAME, ORDERS, SILVER, PRODUCTS_TABLE)\n",
    "\n",
    "GOLD_CLIENTS_ADDRESS_PATH = os.path.join(BUCKET_NAME, ORDERS, GOLD, CLIENTS_ADDRESS_TABLE)\n",
    "GOLD_PRODUCTS_PATH = os.path.join(BUCKET_NAME, ORDERS, GOLD, PRODUCTS_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f3f6cd-b610-4acf-adcd-34a4e81cac3e",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_csv_to_df(file_path, schema=None):\n",
    "    \"\"\"\n",
    "    Read CSV file into DataFrame.\n",
    "    \n",
    "    :param file_path: Path to the JSON file.\n",
    "    :param schema: Optional schema to enforce while reading.\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # Validate file_path\n",
    "    if not isinstance(file_path, str) or not file_path:\n",
    "        raise ValueError(\"Invalid file path provided.\")\n",
    "\n",
    "    # Read DataFrame from CSV\n",
    "    reader = (\n",
    "        spark\n",
    "        .read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .option(\"delimiter\", \",\")\n",
    "        .option(\"escape\", '\"')\n",
    "    )\n",
    "        \n",
    "    if schema:\n",
    "        reader = reader.schema(schema)\n",
    "    else:\n",
    "        reader = reader.option(\"inferSchema\", \"true\")\n",
    "    \n",
    "    try:\n",
    "        df = reader.load(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading CSV file: {str(e)}\")\n",
    "\n",
    "def read_json_to_df(file_path, schema=None):\n",
    "    \"\"\"\n",
    "    Read JSON file into DataFrame.\n",
    "    \n",
    "    :param file_path: Path to the JSON file.\n",
    "    :param schema: Optional schema to enforce while reading.\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # Validate file_path\n",
    "    if not isinstance(file_path, str) or not file_path:\n",
    "        raise ValueError(\"Invalid file path provided.\")\n",
    "\n",
    "    # Read DataFrame from JSON\n",
    "    reader = spark.read.format(\"json\").option(\"multiLine\", \"true\").option(\"mode\", \"PERMISSIVE\")\n",
    "    \n",
    "    if schema:\n",
    "        reader = reader.schema(schema)\n",
    "    else:\n",
    "        reader = reader.option(\"inferSchema\", \"true\")\n",
    "    \n",
    "    try:\n",
    "        df = reader.load(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading JSON file: {str(e)}\")\n",
    "\n",
    "def write_df_to_metastore(df, file_path, table_name, partition_by=None, mode=\"overwrite\"):\n",
    "    \"\"\"\n",
    "    Write DataFrame to Parquet file and save it as a table in the metastore.\n",
    "    \n",
    "    :param df: DataFrame to be written.\n",
    "    :param file_path: Path where the Parquet file will be saved.\n",
    "    :param table_name: Name of the table to save.\n",
    "    :param partition_by: Column(s) to partition by.\n",
    "    :param mode: Write mode, default is 'overwrite'. Other options are 'append', 'ignore', 'error'.\n",
    "    \"\"\"\n",
    "    # Validate parameters\n",
    "    if not file_path or not isinstance(file_path, str):\n",
    "        raise ValueError(\"Invalid file path provided.\")\n",
    "    if not table_name or not isinstance(table_name, str):\n",
    "        raise ValueError(\"Invalid table name provided.\")\n",
    "    if partition_by and not isinstance(partition_by, (str, list)):\n",
    "        raise ValueError(\"Partition by should be a string or a list of strings.\")\n",
    "\n",
    "    writer = df.write.format(\"parquet\").mode(mode).option(\"path\", file_path)\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.saveAsTable(table_name)\n",
    "\n",
    "def transform_clients_bronze_to_silver(clients_df):\n",
    "    \"\"\"\n",
    "    Transform Bronze (raw) Clients DataFrame to Silver (cleaned) DataFrame.\n",
    "    \n",
    "    :param clients_df: Clients DataFrame.\n",
    "    :return: Cleaned Clients DataFrame.\n",
    "    \"\"\"\n",
    "    # Example transformations: Filtering active clients, renaming columns, etc.\n",
    "    clients_silver_df = (\n",
    "        clients_df\n",
    "        .filter(f.col(\"status\") == \"active\")\n",
    "    )\n",
    "    return clients_silver_df\n",
    "\n",
    "def transform_addresses_bronze_to_silver(addresses_df):\n",
    "    \"\"\"\n",
    "    Transform Bronze (raw) Addresses DataFrame to Silver (cleaned) DataFrame.\n",
    "    \n",
    "    :param addresses_df: Addresses DataFrame.\n",
    "    :return: Cleaned Addresses DataFrame.\n",
    "    \"\"\"\n",
    "    # Cast coordenates format to float\n",
    "    addresses_silver_df = (\n",
    "        addresses_df\n",
    "        #.filter(col(\"house_number\") != \"\")\n",
    "        .withColumn(\"lat\", f.col(\"lat\").cast(\"float\"))\n",
    "        .withColumn(\"lon\", f.col(\"lon\").cast(\"float\"))\n",
    "    )\n",
    "    return addresses_silver_df\n",
    "\n",
    "\n",
    "def transform_clients_addresses_silver_to_gold(clients_silver_df, addresses_silver_df):\n",
    "    \"\"\"\n",
    "    Transform Silver (cleaned) Clients and Addresses DataFrames to Gold (aggregated/enriched) DataFrame.\n",
    "    \n",
    "    :param clients_silver_df: Cleaned Clients DataFrame.\n",
    "    :param addresses_silver_df: Cleaned Addresses DataFrame.\n",
    "    :return: Enriched DataFrame combining both clients and addresses.\n",
    "    \"\"\"\n",
    "    # Example aggregation: Joining clients with their addresses\n",
    "    gold_df = (\n",
    "        clients_silver_df\n",
    "        .join(addresses_silver_df, on=\"client_id\", how=\"right\")\n",
    "        .dropna(subset=['client_id'])\n",
    "    )\n",
    "    return gold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd89a210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def pounds_to_kg(pounds):\n",
    "    try:\n",
    "        return float(pounds) * 0.45359237\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def ounces_to_kg(ounces):\n",
    "    try:\n",
    "        return float(ounces) * 0.0283495\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def transform_products_bronze_to_silver(products_df, min_stock=75):\n",
    "    \"\"\"\n",
    "    Transform Bronze (raw) Products DataFrame to Silver (cleaned) DataFrame.\n",
    "    \n",
    "    :param products_df: Products DataFrame.\n",
    "    :return: Cleaned Productss DataFrame.\n",
    "    \"\"\"\n",
    "    # Declare UDF function\n",
    "    convert_to_kg_udf = f.udf(lambda x: convert_to_kg(x), t.DoubleType())\n",
    "    # Define UDFs\n",
    "    pounds_to_kg_udf = f.udf(pounds_to_kg, t.FloatType())\n",
    "    ounces_to_kg_udf = f.udf(ounces_to_kg, t.FloatType())\n",
    "    products_silver_df = (\n",
    "        products_df\n",
    "        # Filter missing values from \n",
    "        .filter(f.col(\"Shipping Weight\").isNotNull())\n",
    "        # Add prefix to product_id column\n",
    "        .withColumn(\"product_id\", f.concat(f.lit(\"pro-\"), f.col(\"Uniq Id\")))\n",
    "        # Cast Price column\n",
    "        .withColumn(\"price\", f.regexp_replace(f.col(\"Selling Price\"), r'\\$', ''))\n",
    "        # Cast & Convert weight units to kg\n",
    "        .withColumn(\"cleaned_weight\", f.trim(f.col(\"Shipping Weight\")))\n",
    "        .withColumn(\"value\", f.regexp_extract(f.col(\"cleaned_weight\"), r\"(\\d+\\.?\\d*)\", 1))\n",
    "        .withColumn(\"unit\", f.regexp_extract(f.col(\"cleaned_weight\"), r\"(pounds|ounces)\", 1))\n",
    "        .withColumn(\n",
    "            \"weight\",\n",
    "            f.when(f.col(\"unit\") == \"pounds\", f.round(pounds_to_kg_udf(f.col(\"value\")),2))\n",
    "            .when(f.col(\"unit\") == \"ounces\", f.round(ounces_to_kg_udf(f.col(\"value\")),2))\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        # Add Random Quantity column\n",
    "        .withColumn('quantity', (min_stock + f.round(f.rand(seed=43)*42, 0)))\n",
    "        # Add timestamp\n",
    "        .withColumn(\"updated_at\", f.current_timestamp())\n",
    "        # select \n",
    "        .select(\n",
    "            f.col(\"product_id\"), \n",
    "            f.col(\"Product Name\").alias(\"product_name\"), \n",
    "            f.col(\"Category\").alias(\"category\"),\n",
    "            f.col(\"price\").cast(t.DoubleType()),\n",
    "            f.col(\"weight\").cast(t.DoubleType()),\n",
    "            f.col(\"quantity\").cast(t.IntegerType()),\n",
    "            f.col(\"Product Specification\").alias(\"product_details\"),\n",
    "            f.col(\"updated_at\")\n",
    "        )\n",
    "        # Drop rows with NaN values in 'price' or 'weight' columns\n",
    "        .dropna(subset=[\"price\", \"weight\"], how=\"any\")\n",
    "    )\n",
    "    return products_silver_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3966786-7aa7-4a8b-8901-76938b8888d7",
   "metadata": {},
   "source": [
    "## 2. Medallion Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b71d1-756f-4823-964d-b0d2431aef03",
   "metadata": {},
   "source": [
    "### 2.1 Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7a55b6-d181-4646-98c6-a6764c218bed",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the clients JSON structure\n",
    "clients_schema = t.StructType([\n",
    "    t.StructField(\"client_id\", t.StringType(), False),\n",
    "    t.StructField(\"first_name\", t.StringType(), False),\n",
    "    t.StructField(\"last_name\", t.StringType(), False),\n",
    "    t.StructField(\"email\", t.StringType(), True),\n",
    "    t.StructField(\"phone_number\", t.StringType(), True),\n",
    "    t.StructField(\"date_of_birth\", t.DateType(), True),\n",
    "    t.StructField(\"gender\", t.StringType(), True),\n",
    "    t.StructField(\"occupation\", t.StringType(), True),\n",
    "    t.StructField(\"created_at\", t.DateType(), True),\n",
    "    t.StructField(\"updated_at\", t.DateType(), True),\n",
    "    t.StructField(\"status\", t.StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the addresses JSON structure\n",
    "addresses_schema = t.StructType([\n",
    "    t.StructField(\"client_id\", t.StringType(), False),\n",
    "    t.StructField(\"address_id\", t.StringType(), False),\n",
    "    t.StructField(\"neighborhood\", t.StringType(), True),\n",
    "    t.StructField(\"coordinates\", t.ArrayType(t.DoubleType()), True),\n",
    "    t.StructField(\"road\", t.StringType(), True),\n",
    "    t.StructField(\"house_number\", t.StringType(), True),\n",
    "    t.StructField(\"suburb\", t.StringType(), True),\n",
    "    t.StructField(\"city_district\", t.StringType(), True),\n",
    "    t.StructField(\"state\", t.StringType(), True),\n",
    "    t.StructField(\"postcode\", t.StringType(), True),\n",
    "    t.StructField(\"country\", t.StringType(), True),\n",
    "    t.StructField(\"lat\", t.StringType(), True),\n",
    "    t.StructField(\"lon\", t.StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the products CSV structure\n",
    "products_schema = t.StructType([\n",
    "    t.StructField('Uniq Id', t.StringType(), True), \n",
    "    t.StructField('Product Name', t.StringType(), True), \n",
    "    t.StructField('Brand Name', t.StringType(), True), \n",
    "    t.StructField('Asin', t.StringType(), True), \n",
    "    t.StructField('Category', t.StringType(), True), \n",
    "    t.StructField('Upc Ean Code', t.StringType(), True), \n",
    "    t.StructField('List Price', t.StringType(), True), \n",
    "    t.StructField('Selling Price', t.StringType(), True), \n",
    "    t.StructField('Quantity', t.StringType(), True), \n",
    "    t.StructField('Model Number', t.StringType(), True), \n",
    "    t.StructField('About Product', t.StringType(), True), \n",
    "    t.StructField('Product Specification', t.StringType(), True), \n",
    "    t.StructField('Technical Details', t.StringType(), True), \n",
    "    t.StructField('Shipping Weight', t.StringType(), True), \n",
    "    t.StructField('Product Dimensions', t.StringType(), True), \n",
    "    t.StructField('Image', t.StringType(), True), \n",
    "    t.StructField('Variants', t.StringType(), True), \n",
    "    t.StructField('Sku', t.StringType(), True), \n",
    "    t.StructField('Product Url', t.StringType(), True), \n",
    "    t.StructField('Stock', t.StringType(), True), \n",
    "    t.StructField('Product Details', t.StringType(), True), \n",
    "    t.StructField('Dimensions', t.StringType(), True), \n",
    "    t.StructField('Color', t.StringType(), True), \n",
    "    t.StructField('Ingredients', t.StringType(), True), \n",
    "    t.StructField('Direction To Use', t.StringType(), True), \n",
    "    t.StructField('Is Amazon Seller', t.StringType(), True), \n",
    "    t.StructField('Size Quantity Variant', t.StringType(), True), \n",
    "    t.StructField('Product Description', t.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42300c19-3aba-4e6f-840b-904dd75f8d91",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_address_raw = read_json_to_df(RAW_ADDRESS_PATH, addresses_schema)\n",
    "df_clients_raw = read_json_to_df(RAW_CIENTS_PATH, clients_schema)\n",
    "df_products_raw = read_csv_to_df(RAW_PRODUCTS_PATH, products_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7f486c-fb34-4881-a75c-fe2615632edd",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_df_to_metastore(df_address_raw, BRONZE_ADDRESS_PATH, \"bronze_address_table\")\n",
    "write_df_to_metastore(df_clients_raw, BRONZE_CLIENTS_PATH, \"bronze_clients_table\")\n",
    "write_df_to_metastore(df_products_raw, BRONZE_PRODUCTS_PATH, \"bronze_products_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8e2bd-a15c-4798-b2aa-269fd099710e",
   "metadata": {},
   "source": [
    "### 2.2 Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc7a764-746e-4d97-ab87-f51d5a89f611",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_address_bronze = spark.table(\"bronze_address_table\")\n",
    "df_clients_bronze = spark.table(\"bronze_clients_table\")\n",
    "df_products_bronze = spark.table(\"bronze_products_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3de0d8-5a88-4f6f-8bfc-2483c1a693c7",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_df_to_metastore(transform_addresses_bronze_to_silver(df_address_bronze), SILVER_ADDRESS_PATH, \"silver_address_table\")\n",
    "write_df_to_metastore(transform_clients_bronze_to_silver(df_clients_bronze), SILVER_CLIENTS_PATH, \"silver_clients_table\")\n",
    "write_df_to_metastore(transform_products_bronze_to_silver(df_products_bronze), SILVER_PRODUCTS_PATH, \"silver_products_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ea14a-e5a6-4257-8e46-35bbb8d86b35",
   "metadata": {},
   "source": [
    "### 2.3 Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0d155f-d848-42d6-ada6-73471bce7ee2",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_address_silver = spark.table(\"silver_address_table\")\n",
    "df_clients_silver = spark.table(\"silver_clients_table\")\n",
    "df_products_silver = spark.table(\"silver_products_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266c0005-8405-4d94-9ec2-204c195c94d4",
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_df_to_metastore(transform_clients_addresses_silver_to_gold(df_clients_silver, df_address_silver), GOLD_CLIENTS_ADDRESS_PATH, \"gold_clients_address_table\")\n",
    "write_df_to_metastore(df_products_silver, GOLD_PRODUCTS_PATH, \"gold_products_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5c1573-898a-4617-b583-dd9a51e0adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- product_details: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df_products_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08cc8c-700a-4ef7-8472-3d192e5d2921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
